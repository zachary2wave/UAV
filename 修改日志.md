# 版本列表

[TOC]

##  ddpg—multioutput

针对2d环境

将action 调整为2个输出口 结果不好

## ddpf——with-warm-start

针对2d环境

这个版本是增加了 warm-start 和 L2 约束 

2019-7-10 训练结果是 UAV 不再 碰撞边界，但是原地不动 可能因为奖励设计不好。
#### 2019-7-7  add the l2 regulaztion 
梯度不收敛 ，没有什么效果 。但是发现 ，warm 开始学习并可以达到不错的效果。
#### 2019-7-9  the gamma  = 0.9  and  lager the positive reward 

发现刚开始梯度上升 ，可以得到比较好的结果 ，但是最后梯度下降了之后，反而得到了不好的结果
这个意味着什么？？

2019-7-10 add the batch normaliztion 

出错 

```python 
Traceback (most recent call last):
  File "/home/zachary/Python程序/UAV/ddpf—with-warm-start.py", line 147, in <module>
    verbose=2, nb_max_episode_steps=1000,
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/rl/core.py", line 670, in warm_fit
    metrics = self.warmup_backward(reward, terminal=done)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/rl/agents/ddpg.py", line 420, in warmup_backward
    self.actor.train_on_batch(inputs,action_batch)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py", line 1211, in train_on_batch
    class_weight=class_weight)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py", line 751, in _standardize_user_data
    exception_prefix='input')
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_utils.py", line 92, in standardize_input_data
    data = [standardize_single_array(x) for x in data]
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_utils.py", line 92, in <listcomp>
    data = [standardize_single_array(x) for x in data]
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_utils.py", line 27, in standardize_single_array
    elif x.ndim == 1:
AttributeError: 'bool' object has no attribute 'ndim'
```

可能是因为  输入了  self.training   在train on batch上

增加了batchnormaliazation 之后  出现下面错误： 

```python
Traceback (most recent call last):
  File "/home/zachary/Python程序/UAV/ddpf—with-warm-start.py", line 165, in <module>
    verbose=2, nb_max_episode_steps=1000)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/rl/core.py", line 201, in fit
    metrics = self.backward(reward, terminal=done)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/rl/agents/ddpg.py", line 328, in backward
    action_values = self.actor_train_fn(inputs)[0]
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2715, in __call__
    return self._call(inputs)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1399, in __call__
    run_metadata_ptr)
  File "/home/zachary/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 526, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'observation_input' with dtype float and shape [?,1,19]
	 [[{{node observation_input}} = Placeholder[dtype=DT_FLOAT, shape=[?,1,19], _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
	 [[{{node model_1/concatenate_1/concat/_523}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1358_model_1/concatenate_1/concat", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

```

#### 2019-7-11 

训练结果中出现较多的已完成 。但是不知道是不是最优。从测试结果来看这个地方还没达到可以完成地步。

**分析**：

**改变** 重新 加载之前的模型继续训练。此时不再warm-up的

现象 训练出现了非常大的负值， 推测

主要原因是 位置变化导致 其又不知道怎么玩了。



#### 2019-7-16

把G也固定了  看看效果
#### 2019-7-18
固定G之后 可以出优化比较好的结果，完成的也比较好。
设定奖励在3以内 
然后loss下降了，

目前发现问题就是，当预训练到 loss很小的时候， 再训练 loss会继续下降，没有上升趋势
但是如果训练到一定程度 再开始 那么loss 还是会接着下降 不会出现上升趋势
也就是说当出现了负面样本的时候 为什么没有loss上升

如果单纯用训练的话 ， 根本学不会。 

采用模仿学习和自我探索相结合的套路来弄  看看行不行








## ddpg-2d-v3 

针对2d环境

针对 V3 版本， 在warm-start的基础上又增加了 batch normalization



## imitnation-fit-in -roll

针对2d环境 V3 版本， 

进行循环的训练，看看效果

#### 2019-7-23日

选择循环的进行 模仿学习和 强化学习，最后 结果也不是很好。

首先观察到的现象是，

同时观察到随机性，

#### 2019-7-25

选择循环的进行 模仿学习和 强化学习，最后 结果也不是很好。

首先观察到的现象 在模仿学习结束后， 强化学习输出的结果并不是很好

强化学习学习到一定的程度之后 就会停止。 感觉是环境的问题。

#### 2019-7-26

load 数据之后  感觉不对

所以采用不load 权重，

可以看出  在模仿训练几次之后大概3到4个回合之后 就可以完成任务  但是一开是强化学习效果就不对了 

然后开始

这个地方问题在哪里？？？？？

#### 2019-7-29

感觉还是指向性有问题　

修改了 reward  看看结果

#### 2019-7 -30

reward 设计的也是不对的  

但是新的reward设计之后也没有感觉到新的reward 有什么增加  同样也是跑不出来

反而是老的reward  更能完成任务。为老的reward 增加

之前gamma 都是0.1 loss 下降的很快 

#### 2019-7-31

gamma 设计为 0.9 之后  昨天看到了比较好的结果，今天设计了早停 达到了预期的目标。
但是发现一个问题，就是 达到了这个目标之后 最后并不是最优的
所以 明天开始 重新设计reward  
今天接着干一件事儿 就是  在这个参数下 可以不可以reset 尝试

在V5 环境下 得到结果
#### 2019-8-1

在V5 环境下 得到结果 有训练的比较好的结果 但是总体上来看 还是有问题。
今天写一个测试 reward的 看看是不是对应的结果。
#### 2019-8-2

在V5 环境下 得到结果 有训练的比较好的结果 但是总体上来看 还是有问题。
今天写一个测试 reward的 看看是不是对应的结果。

#### 2019-8-3

新测试了一个奖励 最后的结果看起来有一些 但是训练时间过长 然后就发散了
现在


## generate——warm——up

这边是测试环境用的 ，也可以生成 warm-up 数据

## polic-test——2d

这边是测试环境用的 